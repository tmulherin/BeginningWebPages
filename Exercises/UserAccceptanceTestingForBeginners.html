<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="UTF-8"/>
        <title>User Acceptance Testing for Beginners</title>
    </head>
    <body>
    <h3 align="center">User Acceptance Testing for Beginners<br />
                A handbook</h3>

<!-- Table of Contents -->    

    <div id="contents"><a id="top"> </a>
        <h4>Table of Contents</h4>
        <a href="#overview">User Acceptance Testing (UAT) Overview</a><br />
        <a href="#artifacts">Artifacts</a><br />
        <a href="#test_phases">Phases</a><br />
        <a href="#owners">Artifacts and Owners</a><br />
        <a href="#resources">UAT Resources</a><br />
        <a href="#UAT Preperation">UAT Preparation</a><br />
        <a href="#Entrance Criteria">UAT Entrance Criteria</a><br />
        <a href="#Documentation">Preparation / Documentation</a><br />
        <a href="#UAT Testing">UAT Testing</a><br />
        <a href="#Defect Management">Defect Management</a><br />
        <a href="#UAT Testing Evidence">UAT Testing Evidence</a><br />
        <a href="#Go/No Go">Go/No Go</a><br />
        <a href="#UAT Communication Plan">UAT Communication Plan</a><br />
        <a href="#Business Checkout">Business Checkout - Test Preparation / 
                                     Test Documentation</a><br />
        <a href="#Release Housekeeping">Release Housekeeping</a><br />
        <a href="#Where Test">Where should I be testing?</a><br />
        <a href="#Glossary">Glossary of terms</a><br />
    </div>

<!-- User Acceptance Testing (UAT) Overview -->    

    <div><a id="overview"> </a>
        <hr /> 
        <h3>User Acceptance Testing (UAT) Overview</h3>
        <hr />
        <p>User Acceptance Testing is very important to the Business.  It helps 
           ensure successful delivery of new applications and/or functionality 
           by testing the applications in the context of the business 
           environment in which it will operate.  The expectation is that if 
           software works as intended during simulation of normal use, it will 
           work the same in production.</p>  
        <p>Application end-users perform UAT.  They understand the business and 
            can effectively evaluate if application changes function as 
            expected during normal business usage.</p>   
        <p>UAT is designed to verify performance of critical business functions 
           and integrity of converted and additional data prior to checkout 
           weekend when system changes are moved to production.  UAT is 
           performed after QA has completed tests designed to flush out show 
           stoppers such as system crashes.  The business end-user sign offs 
           that the system is ready to “Go Live”.</p>
        <small><a href="#top">contents</a></small>
    </div>

<!-- Test Artifacts -->    

    <div><a id="artifacts"> </a>
        <hr />
        <h3>Test Artifacts</h3>
        <hr />
        <p>QA develops the following test artifacts 2-3 months in advance of  
           scheduled releases based on size and complexity of release.  There 
           are three categories of artifacts.</p>
        <ol id="artifact_categories">
            <!-----------------------------------------------------------------
              ==> 'type' is deprecated 
              ==> Change from ordered list.  
              ==> The categories are incomplete. (reqs, defects, cycles, etc.) 
              ==> The first two apply to the QA Engineer, not the SME 
            ------------------------------------------------------------------>
            <li>The test strategy outlines how testing for the release will be 
                accomplished.  This includes:  technical tools/environments; 
                necessary test data and its creation; business scenarios 
                (provided by the project Business SMEs); and identification of 
                necessary test cases.  If help is required to get the UAT 
                Testers up to speed on changes based on the number and/or 
                complexity of changes, this will be specifically outlined in 
                the strategy. It is generally a high level document that 
                identifies “what” you are going to test.</li>  
            <li>The test plan outlines the test phases, associated tasks, and 
                assigned resources. It is more detailed and generally defines 
                “how” you are going to test.</li>
            <li>The test cases outline each step necessary to execute the 
                testing nd specify actions and expected results.  QA generally 
                performs walkthroughs with the business SMEs to ensure all 
                major functions are covered.</li>
        </ol>
        <p>The inputs for the creation of these documents would be based on use 
           cases, business requirements, technical specifications and any 
           review comments from the business team/SME.</p>
        <p>QA reviews the test plan with the core project team, makes necessary 
           updates, and receives sign off from the core team.  The application 
           Project Manager then communicates the plan to all 
           affected parties.</p>
        <small><a href="#top">contents</a></small>
    </div>

<!-- Test Phases -->    

    <div><a id="test_phases"> </a>
        <hr />
        <h3>Test Phases</h3>
        <hr />
        <p>There are 4 phases of testing:  Unit; Systems; <b>Acceptance</b>; 
           and Release.</p>
        <table id="tbl_test_phases" 
               border="1"
               align="center"
               cellpadding="5"
               cellspacing="1">
            <tr>
                <th>Test Phase</th>
                <th>Test</th>
                <th>Owner/Executor</th>
                <th>Purpose</th>
            </tr>
            <tr>
                <td headers="Test Phase" rowspan="2">Unit</td>
                <td headers="Test">Usability</td> <!-- Is this a unit test or 
                                                       an integration test? -->
                <td headers="Owner/Executor">Developers</td>
                <td headers="Purpose">Validate design meets user goals and ease 
                                      of use</td>
            </tr>
            <tr>
                <td headers="Test">Unit</td>
                <td headers="Owner/Executor">Developers</td>
                <td headers="Purpose">Test individual modules and programs</td>
            </tr>
            <tr>
                <td headers="Test Phase" rowspan="3">Systems</td> 
                <td headers="Test">Function</td>
                <td headers="Owner/Executor">QA</td>
                <td headers="Purpose">Use non-integrated data to validate 
                                      changes made to an application are 
                                      functioning correctly.
                                       <!-- Non-integrated data> -->
                </td>
            </tr>
            <tr>
                <td headers="Test">Integration</td>
                <td headers="Owner/Executor">QA</td>
                <td headers="Purpose">Use integrated data to validate changes 
                                      work with impacted applications and 
                                      ensure applications interfacing correctly.
                </td>
            </tr>
            <tr>
                <td headers="Test">Regression</td>
                <td headers="Owner/Executor">QA</td>
                <td headers="Purpose">Evaluate interactions between components 
                                      to determine impact of changes on target 
                                      environments.
                </td>
            </tr>
            <tr>
                <td headers="Test Phase"><b>Acceptance</b></td>
                <td headers="Test">Acceptance</td>
                <td headers="Owner/Executor">Business user<br />
                    acceptance testers</td>
                <td headers="Purpose">Validate that new business functionality 
                                      and applications work correctly under 
                                      normal business workflows.</td>
            </tr>
            <tr>
                <td headers="Test Phase" rowspan="2">Release</td>
                <td headers="Test">Release</td>
                <td headers="Owner/Executor">QA</td>
                <td headers="Purpose">Use frozen code to validate that new and 
                                      existing functionality work correctly.
                </td>
            </tr>
            <tr>
                <td headers="Test">Business Checkout</td>
                <td headers="Owner/Executor">UW - QA w/ business backup<br />
                    Sales – Sales Effectiveness <!-- what is this? -->
                </td>
                <td headers="Purpose">Validate that new business functionality 
                                      and applications have been successfully 
                                      moved to production.</td>
            </tr>
        </table>
        <small><a href="#top">contents</a></small>
    </div>

<!-- Artifacts and Owners -->    

    <div><a id="owners"> </a>
        <hr />
        <h3>Artifacts and Owners</h3>
        <hr />
        <p>In order to perform the required testing, artifacts are required.  
           The below table outlines this information by test phase</p>
        <table id="Artifact and Owners" 
                border="1"
                align="center"
                cellpadding="5"
                cellspacing="1">
            <tr>
                <th>Phase</th>
                <th>Tests</th>
                <th>Inputs</th>
                <th>Creator</th>
                <th>Outputs</th>
                <th>Owner</th>
                <th>Sign-off</th>
            </tr>
            <tr>
                <td rowspan="2"
                    headers="Phase">Unit</td>
                <td headers="Tests">Usability</td>
                <td rowspan="2"
                    headers="Inputs">Test Cases</td>
                <td rowspan="2"
                    headers="Creator">Developers</td>
                <td rowspan="2"
                    headers="Outputs"> </td>
                <td rowspan="2"
                    headers="Owner">QA</td><!-- Why is QA the owner? -->
                <td rowspan="2"
                    headers="Sign-off">QA</td>
            </tr>
            <tr>
                <td headers="Tests">Unit</td>
            </tr>
            <tr>
                <td rowspan="3"
                    headers="Phase">Systems Test</td>
                <td headers="Tests">Functional</td>
                <td rowspan="3"
                    headers="Inputs">Test Cases</td>
                <td rowspan="3"
                    headers="Creator">QA</td>
                <td rowspan="3"
                    headers="Outputs">Test results</td>
                <td rowspan="3"
                    headers="Owner">QA</td>
                <td rowspan="2"
                    headers="Sign-off">QA</td>
            </tr>
            <tr>
                <td headers="Tests">Integration</td>
            </tr>
            <tr>
                <td headers="Tests">Regression</td>
                <td headers="Owner">Business SME</td>
            </tr>
            <tr>
                <td rowspan="2"
                    headers="Phase">Acceptance Test</td>
                <td rowspan="2"
                    headers="Tests">User Acceptance Testing</td>
                <td headers="Inputs">Test Scenarios</td>
                <td rowspan="2">Business SME</td>
                <td rowspan="2"
                    headers="Outputs">Test results</td>
                <td rowspan="2"
                    headers="Owner">Business/UAT Testers</td>
                <td rowspan="2"
                    headers="Sign-off">Business SME</td>
            </tr>
            <tr>
                <td headers="Inputs">Test Cases</td>
            </tr>
            <tr>
                <td rowspan="3"
                    headers="Phase">Release Test</td>
                <td headers="Tests">Release</td>
                <td headers="Inputs">Test Cases</td>
                <td headers="Creator">QA</td>
                <td headers="Outputs"> </td>
                <td headers="Owner">QA</td>
                <td headers="Sign-off" > </td>
            </tr>
            <tr>
                <td rowspan="2"
                    headers="Tests">Business Checkout</td>
                <td headers="Inputs">Test Scenarios</td>
                <td rowspan="2"
                    headers="Creator">Business SME</td>
                <td rowspan="2"
                    headers="Outputs">Test results</td>
                <td rowspan="2"
                    headers="Owner">QA</td>
                <td rowspan="2"
                    headers="Sign-off">Business SME</td>
            </tr>
            <tr>
                <td headers="Inputs">Test Cases</td>
            </tr>
        </table>
        <small><a href="#top">contents</a></small>
    </div>

<!-- Resources -->    

    <div><a id="resources"> </a>
        <hr />
        <h3>Resources</h3>
        <hr />
        <p>Business resources performing UAT will vary depending on the 
           application and functionality:</p>

        <p>Note: We are in the process of identifying/validating the business 
           resources for each application.</p>
        <small><a href="#top">contents</a></small>  
    </div>
    <div><a id="UAT Preperation"> </a>
        <hr />
        <h3>UAT Preparation</h3>
        <hr />
        <p>Prior to executing User Acceptance Testing, the following needs to 
           be completed: </p>
        <ol>
            <li><u>Data Setup</u>
                <p><b>Task:</b>  Populate the test environment with test data 
                                 based on business scenarios provided by the 
                                 project Business Application SMEs and outlined 
                                 in the QA test plan.  Examples include:  
                                 accounts, opportunities, and quotes.<br />
                <b>Lead Time:  </b>TBD<br />
                <b>Owner:  </b>QA<br />
                <i>Pre-requisite: SME to provide the list of UAT testers to 
                   QA.</i></p>
            </li>
            <li><u>Access to Applications</u>
                <p><b>Task:  </b>Ensure UAT test ids are set up with the 
                                 appropriate rights.<br />
                <b>Lead Time:  </b>TBD<br />
                <b>Owner:  </b>QA and Tech Services Team
                <p><b>Task:  </b>Ensure testers have access to the applications 
                                 being tested<br />
                <b>Lead Time:  </b>TBD<br />
                <b>Owner:  </b>Business Application SME</p>
                <p><b>Task:  </b>Ensure UAT Test Coordinator has access to the 
                                 defect tracking system<br />
                <b>Lead Time:  </b>TBD<br />
                <b>Owner:  </b>QA</p>
            </li>
        </ol>
        <p><i>Note: Each of the above activities would be listed as activities 
           within the QA project plan</i></p>
        <small><a href="#top">contents</a></small>
    </div>

<!-- Entrance Criteria -->    

    <div><a id="Entrance Criteria"> </a>
        <hr />
        <h3>Entrance Criteria</h3>
        <hr />
        <p>User Acceptance testing is typically performed near the end of the 
           test phase. It is done once the QA testers have completed their 
           functional, integration, and the majority of the regression testing 
           and presented test evidence to the Business application subject 
           matter experts (SMEs) for sign-off.  QA will send an email 
           notification to User Acceptance testers to start testing.</p>
        <ul>
            <li>It is best to not have any open critical or major severity 
                defects.  Assessment of open defects must be made and 
                documented to determine UAT GO/NO GO decision.</li>
            <li>A defect log has been created.  Next steps on all defects have 
                been agreed upon by the business and IT.</li>
            <li>A Defect Risk Assessment of defects has been performed and 
                documented.</li>
            <li>Outstanding issues pertaining to the Systems Test phases and an 
                Issues Risk Assessment has been performed and documented.</li>
            <li>Key Systems Development Life Cycle (SDLC) deliverables impacted 
                by the System Test phase have been updated:</li>
            <ul>
                <li>Business Requirements</li>
                <li>Technical Design</li>
                <li>Test Strategy</li>
                <li>Test Plan</li>
                <li>Issues Log</li>
                <li>Risk Assessment</li>
                <li>Integrated Test Cases</li>
                <li>System Test Cases</li>
            </ul>
            <li>Acceptance Environment is stable.</li>
            <li>All required code for the release has been deployed to the 
                Acceptance Test environment.</li>
            <li>Access to the UAT applications for the UAT testers has been 
                established.</li>
        </ul>
        <small><a href="#top">contents</a></small>
    </div>

<!-- Test Preparation / Test Documentation -->    

    <div><a id="Documentation"> </a>
        <hr />
        <h3>Test Preparation / Test Documentation</h3>
        <hr />
        <p><b>Task:  </b>Plan a UAT Training session<br />
            <b>Lead Time:  </b>TBD<br />
            <b>Owner:  </b>Business UAT Lead<br /><br />
            <b>Task:  </b>Write high level business summary (ex. Release notes) 
                          of the application changes being implemented in the 
                          release.Plan a UAT Training session<br />
            <b>Lead Time:  </b>TBD<br />
            <b>Owner:  </b>Business Application SME and SA<br /><br />
            <b>Task:  </b>Prepare for design review with UAT testers (ex. Demo 
                          of application changes)<br />
            <b>Lead Time:  </b>TBD<br />
            <b>Owner:  </b>SA<br /><br />
            <b>Task:  </b>User Acceptance Testers participate in a release 
                          training session.  Training session will 
                          include:
            <ul>
                <li>Review of UAT expectations</li>
                <li>Review of UAT Handbook</li>
                <li>SA presentation of the changes to the application(s) 
                    allowing User Acceptance Testers to ask questions and view 
                    the application in the test environment.</li>
                <li>Break out sessions for identifying business scenarios to 
                    test the range of old and new functionality.  Supported by 
                    SA, QA, and Business Application SMEs. Note: The business 
                    scenarios will be used to create the test data in the test 
                    environments.  Refer to Data Setup in the previous section.
                </li>
                <li>Prioritization of business scenarios identifying which 
                    scenarios are critical to the success of the release.</li>
                <li>Development of test cases that identify the functionality 
                    that will be tested, including the new functionality.  As 
                    appropriate, include integrated end to end test cases 
                    across roles and applications.</li>
            </ul>
            <i>Sample:</i><strong> Embed Excel Spreadheet here</strong><br />
            <b>Lead Time:  </b>TBD<br />
            <b>Owner:  </b>Business UAT Lead, Business Application SME’s, SA’s, 
                           QA, and UAT Testers
         </p>
         <small><a href="#top">contents</a></small>
    </div>

<!-- UAT Testing -->    

    <div><a id="UAT Testing"> </a>
        <hr />
        <h3>UAT Testing</h3>
        <hr />
        <p>The extent of UAT depends on the type of release and will be 
           performed by the user acceptance testers:</p>
        <ol>
            <li><strong>Scheduled Release</strong> – Any new functionality or 
                                                     pre-defined set of 
                                                     enhancements /defect fixes.
                                                     <br />
                <p>For scheduled releases testing is detailed and typically 
                   entails:</p>
                   <ul>
                       <li>Thorough testing of the new business functionality.
                       </li>
                       <li>Validating that enhancements haven’t broken existing 
                           critical business flows that Sales/Underwriting uses 
                           to fulfill the business commitments</li>
                   </ul>
            </li><br />
            <li><strong>Hot Fix Release</strong> – Mitigate or fix critical 
                                                issues.<br />
            <p>Testing for hot fixes are typically less comprehensive and 
               entails:</p>
               <ul>
                   <li>Thorough testing of the actual fixes going in</li>
                   <li>A quick check of existing critical business flows that 
                       Sales/Underwriting uses to fulfill the business 
                       commitments</li>
               </ul>  

            </li>
        </ol>
        <small><a href="#top">contents</a></small>
    </div>

<!-- Defect Management -->    

    <div><a id="Defect Management"> </a>
        <hr />
        <h3>Defect Management</h3>
        <hr />
        <p>Defect management is a critical part of UAT.  Defects are defined as 
           any function/feature of the application that does not meet the 
           defined specifications.  Enhancements are requests for functionality 
           in line with current specifications but improve business flow or 
           performance.</p>

        <p>Defects will be triaged in a daily application defect meeting and 
            should be tracked to closure.  They will either get fixed or be 
            deferred to a future release.  More complex releases will require 
            an overall release triage meeting that will meet as needed.  This 
            meeting is designed to identify critical dependencies across 
            applications in order to appropriately prioritize defect 
            resolution.</p>
        <h4><em>Defect Tracking</em></h4>
        <p><b>Task:  </b>Report defects to the Business Application SME.  
                         Assign Severity and Priority to each defect.<br />
            <b>Sample: Embed Excel Spreadheet here</b> 
            <table>
                <tr>
                    <td><b>Definitions:</b></td>
                    <td>Severity</td></tr>
                <tr>
                    <td rowspan="4">&nbsp;</td>
                    <td><b>1 - Critical</b></td>
                    <td>the defect has caused all testing of that particular 
                        test script to cease; the defect must be fixed 
                        ASAP.</td>
                </tr>
                <tr>
                    <td><b>2 - Major</b></td>
                    <td>the defect has a major impact on the system; however, 
                        there is a temporary work around such that testing does 
                        not have to cease; the defect should be fixed ASAP.</td>
                </tr>
                <tr>
                    <td><b>3 - Minor</b></td>
                    <td>the defect has a minor impact on the system; however, 
                        there is a work around and testing does not have to 
                        cease.</td>
                </tr>
                <tr>
                    <td><b>4 - Cosmetic</b></td>
                    <td>the defect requires a “cosmetic” or “nice to have” 
                        fix.</td>
                </tr>
            </table>
            <p><b>Owner:  </b>User Acceptance Tester<br /></p>
 
            <p><b>Task:  </b>Validate and consolidate defects, adjust severity 
                and priority as appropriate at the application level, log in 
                defect tracking tool, and report to QA Lead<br />
               <b>Owner:  </b>Business Application SME</p>

            <p><b>Task:  </b>Track application defects in Quality Center<br />
               <b>Owner:  </b>QA Lead</p>

            <h4><em>Triage Meetings</em></h4>

            <p><b>Task:  </b>Schedule defect triage meetings with application 
                PM, Lead Developer, SA Lead, and Business Application SME

            <p><b>Task:  </b>Facilitate defect triage meetings to validate 
                defects, assess development effort, prioritize defects, and 
                track decisions in TBD.  Note: Any e-mail threads that occur 
                outside of the triage meeting regarding defects should be kept 
                to a minimum.  When required to facilitate resolution, the QA 
                lead should be copied in on the messages so that the decision 
                thread can be captured in Mercury Quality Center.  It is best 
                practice to call a meeting when the communication requires more 
                than three participants and more than three messages.<br />
               <b>Owner:  </b>Application PM, Lead Developer, and Business 
               Application SME</p>

            <p><b>Task:  </b>Attend QA defect triage meetings, frequency of 
                which depends on number of defects being found, and report out 
                to User Acceptance Testers.<br />
               <b>Owner:  </b>QA Lead</p>

            <p><b>Task:  </b>Schedule overall portfolio release defect triage 
                meetings with application PM’s, Lead Developers, QA Leads, SA 
                Lead, and Business Application SME’s<br />
               <b>Owner:  </b>Coordinated Release QA Lead</p>

            <p><b>Task:  </b>Facilitate overall portfolio release defect triage 
                meetings to assess critical and major severity defects with 
                application interdependencies, assess development effort, 
                prioritize defects, and track decisions in Mercury Quality 
                Center.<br />
               <b>Owner:  </b>Coordinated Release QA Lead</p>

            <p><b>Task:  </b>Attend QA overall release defect triage meetings, 
                frequency of which depends on number of defects being found, 
                and report out to User Acceptance Testers.<br />
               <b>Owner:  </b>Application PM’s, Lead Developers, QA Leads, and 
               Business Application SME’s</p>

            <p><b>Note:  </b>If the application release is part of a larger 
                Integrated Release, the Business Application SME will partner 
                closely with Business Integrated Release SME on defect 
                management.</p>

            <small><a href="#top">contents</a></small>
    </div>

<!-- UAT Testing Evidence -->    
   
    <div><a id="UAT Testing Evidence"> </a>
        <hr />
        <h3>UAT Testing Evidence</h3>
        <hr />
        <p>It is important for User Acceptance Testers to provide QA and the 
            Business Application SME’s with evidence that they performed the 
            required testing.  This is will enable Business Application SME’s 
            to provide sign-off on testing and provide QA with backup data to 
            recreate test scenarios when needed.</p>

        <p><b>Task:  </b>Provide documentation of test cases run and test data 
            used<br />
           <b>Owner:  </b>User Acceptance Testers</p>
        <p><b>Task:  </b>Review documentation of test cases run and test data 
            used provided by each User Acceptance Tester and validate that 
            testing is complete<br />
           <b>Owner:  </b>Business Application SME</p>
  
        <small><a href="#top">contents</a></small>
    </div>

<!-- Go/No Go -->    

    <div><a id="Go/No Go"> </a>
        <hr />
        <h3>Go/No Go</h3>
        <hr />
        <p><b>Task:  </b>User Acceptance Sign-off<br /></p>
        <p>Once user acceptance testing is complete and validated, the Business 
           Application SME will send an email sign-off to the application PM 
           and QA Lead</p>
        <p><b>Owner:  </b>Business Application SME</p>
        <p><b>Task:  </b>Go/No-Go<br /></p>
        <p>he successful exit criteria for UAT would be that there are no 
           Critical or Major defects left unresolved for the release. The 
           Business Owners would review the open defects list for the release 
           and then determine the GO/NO-GO decision.  A GO decision indicates 
           that the application/module is deemed ready to be moved to 
           production.  A NO-GO decision means that the application/module 
           can not be moved to production with the Critical or Major defects 
           left unresolved.</p>
        <p><b>Owner:  </b>Business Application SME’s + Application PM’s</p>    
  
        <small><a href="#top">contents</a></small>
    </div>

<!-- UAT Communication Plan -->    

    <div><a id="UAT Communication Plan"> </a>
        <hr />
        <h3>UAT Communication Plan</h3>
        <hr />
        <p>Communication amongst stakeholders is critical to the success of 
           any release.  Attached is a communication plan that provides 
           guidance on communicating during the UAT test phase.</p>  
 
        <small><a href="#top">contents</a></small>
 
    </div>
    
<!-- Business Checkout - Test Preparation / Test Documentation -->    

    <div><a id="Business Checkout"> </a>
        <hr />
        <h3>Business Checkout - Test Preparation / Test Documentation</h3>
        <hr />

        <h4><em>Small Change / Small Complexity</em></h4>

        <p><b>Task:  </b>Create business scenarios and test cases for checkout 
              that identify the functionality that will be tested, including 
              the new functionality.  QA will execute the checkout test cases 
              and UW will provide Sign-off after reviewing the QA results.<br />
           <b>Owner:  </b>QA / Business Application SME</p>

        <h4><em>Medium or Large Change / Medium or Large Complexity</em></h4>

        <p><b>Task:  </b>Create business scenarios and test cases for checkout 
              that identify the functionality that will be tested, including 
              the new functionality.  These test cases will be executed by QA. 
              The QA team will check for any show stoppers and then give the 
              Business SME’s the go-ahead for business checkout of particular 
              project changes.<br />
           <b>Owner:  </b>QA / Business Application SME</p>

        <small><a href="#top">contents</a></small>
 
    </div>

<!-- Release Housekeeping -->

    <div><a id="Release Housekeeping"> </a>
        <hr />
        <h3>Release Housekeeping</h3>
        <hr />
        <p>Once UAT is complete, QA will document any results for future 
            reference and will send updates of the business scenarios and 
            scripts to the Business Application SME.</p

        <small><a href="#top">contents</a></small>
 
    </div>

<!-- Where should I be testing? -->

    <div><a id="Where Test"> </a>
        <hr />
        <h3>Where should I be testing?</h3>
        <hr />

        <small><a href="#top">contents</a></small>
 
    </div>

<!-- Glossary of Terms -->

    <div>
        <a id="Glossary"> </a>
        <table id="glossary_table" border="1">
            <hr />
            <caption id="tbl_glossary" align="center">Glossary of Terms</caption>
            <hr />
            <thead>
                <tr>
                    <th>Term</th>
                    <th align="left">Definition</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Acceptance Criteria</td>
                    <td>The exit criteria that a component or system must satisfy in order to be accepted by a user, customer, or other authorized entity.</td>
                </tr>
                <tr>
                    <td>Acceptance Testing</td>
                    <td>Formal testing with respect to user needs, requirements, and business processes conducted to determine whether or not a system satisfies the acceptance criteria and to enable the user, customers or other authorized entity to determine whether or not to accept the system.</td>
                </tr>
                <tr>
                    <td>Actual Result</td>
                    <td>The behavior produced/observed when a component or system is tested.</td>
                </tr>
                <tr>
                    <td>Anomaly</td>
                    <td>Any condition that deviates from expectation based on requirements specifications, design documents, user documents, standards, etc. or from someone’s perception or experience. Anomalies may be found during, but not limited to, reviewing, testing, analysis, compilation, or use of software products or applicable documentation.</td>
                </tr>
                <tr>
                    <td>Audit Trail</td>
                    <td>A path by which the original input to a process (e.g. data) can be traced back through the process, taking the process output as a starting point. This facilitates defect analysis and allows a process audit to be carried out</td>
                </tr>
                <tr>
                    <td>Behavior</td>
                    <td>The response of a component or system to a set of input values and preconditions</td>
                </tr>
                <tr>
                    <td>Best Practice</td>
                    <td>A superior method or innovative practice that contributes to the improved performance of an organization under given context, usually recognized as ‘best’ by other peer organizations.</td>
                </tr>
                <tr>
                    <td>Beta Testing</td>
                    <td>Operational testing by potential and/or existing users/customers at an external site not otherwise involved with the developers, to determine whether or not a component or system satisfies the user/customer needs and fits within the business processes. Beta testing is often employed as a form of external acceptance testing in order to acquire feedback from the market.</td>
                </tr>
                <tr>
                    <td>Black Box Testing</td>
                    <td>Testing, either functional or non-functional, without reference to the internal structure of the component or system.</td>
                </tr>
                <tr>
                    <td>Business Process-Based Testing</td>
                    <td>An approach to testing in which test cases are designed based on descriptions and/or knowledge of business processes.</td>
                </tr>
                <tr>
                    <td>Capability Maturity Model (CMM)</td>
                    <td>A five level staged framework that describes the key elements of an effective software process. The Capability Maturity Model covers practices for planning, engineering and managing software development and maintenance.</td>
                </tr>
                <tr>
                    <td>Certification</td>
                    <td>The process of confirming that a component, system or person complies with its specified requirements, e.g. by passing an exam.</td>
                </tr>
                <tr>
                    <td>CCB</td>
                    <td>Change Control Board – controlling body for changes in scope</td>
                </tr>
                <tr>
                    <td>Compliance testing</td>
                    <td>The process of testing to determine the compliance of a component or system.</td>
                </tr>
                <tr>
                    <td>Configuration</td>
                    <td>The composition of a component or system as defined by the number, nature, and interconnections of its constituent parts.</td>
                </tr>
            </tbody>
            <tr>
                <td>Configuration Management</td>
                <td>A discipline applying technical and administrative direction and surveillance to: identify and document the functional and physical characteristics of a configuration item, control changes to those characteristics, record and report change processing and implementation status, and verify compliance with specified requirements.</td>
            </tr>
            <tr>
                <td>Coverage</td>
                <td>The degree, expressed as a percentage, to which a specified coverage item has been exercised by a test suite.</td>
            </tr>
            <tr>
                <td>Data Driven Testing</td>
                <td>A scripting technique that stores test input and expected results in a table or spreadsheet, so that a single control script can execute all of the tests in the table. Data driven testing is often used to support the application of test execution tools such as capture/playback tools(automation).</td>
            </tr>
            <tr>
                <td>Data Validation Testing</td>
                <td>The checking of data for correctness, or the determination of compliance with applicable standards, rules, and conventions.</td>
            </tr>
            <tr>
                <td>Defect</td>
                <td>A flaw in a component or system that can cause the component or system to fail to perform its required function, e.g. an incorrect statement or data definition. A defect, if encountered during execution, may cause a failure of the component or system.</td>
            </tr>
            <tr>
                <td>Defect Management</td>
                <td>The process of recognizing, investigating, taking action and disposing of defects. It involves recording defects, classifying them and identifying the impact.</td>
            </tr>
            <tr>
                <td>Entry Criteria</td>
                <td>The set of generic and specific conditions for permitting a process to go forward with a defined task, e.g. test phase. The purpose of entry criteria is to prevent a task from starting which would entail more (wasted) effort compared to the effort needed to remove the failed entry criteria.</td>
            </tr>
            <tr>
                <td>Exit Criteria</td>
                <td>The set of generic and specific conditions, agreed upon with the stakeholders, for permitting a process to be officially completed. The purpose of exit criteria is to prevent a task from being considered completed when there are still outstanding parts of the task which have not been finished. Exit criteria are used by testing to report against and to plan when to stop testing.</td>
            </tr>
            <tr>
                <td>Expected Result</td>
                <td>The behavior predicted by the specification, or another source, of the component or system under specified conditions.</td>
            </tr>
            <tr>
                <td>Fail</td>
                <td>A test is deemed to fail if its actual result does not match its expected result</td>
            </tr>
            <tr>
                <td>Formal Review</td>
                <td>A review characterized by documented procedures and requirements, e.g. inspection.</td>
            </tr>
            <tr>
                <td>Functional Requirement</td>
                <td>A requirement that specifies a function that a component or system must perform.</td>
            </tr>
            <tr>
                <td>Functional Testing</td>
                <td>Testing based on an analysis of the specification of the functionality of a component or system.</td>
            </tr>
            <tr>
                <td>Integration Testing</td>
                <td>Testing performed to expose defects in the interfaces and in the interactions between integrated components or systems. See also component integration testing, system integration testing.</td>
            </tr>
            <tr>
                <td>Metric</td>
                <td>A measurement scale and the method used for measurement</td>
            </tr>
            <tr>
                <td>Milestone</td>
                <td>A point in time in a project at which defined (intermediate) deliverables and results should be ready.</td>
            </tr>
            <tr>
                <td>Non-Functional Requirement</td>
                <td>A requirement that does not relate to functionality, but to attributes of such as reliability, efficiency, usability, maintainability and portability.</td>
            </tr>
            <tr>
                <td>Pass</td>
                <td>A test is deemed to pass if its actual result matches its expected result.</td>
            </tr>
            <tr>
                <td>Priority</td>
                <td>The level of (business) importance assigned to an item, e.g. defect.</td>
            </tr>
            <tr>
                <td>Performance testing</td>
                <td>The process of testing to determine the performance of a software product. Testing is conducted using some type of performance tool such as Rational Performance Tester or HP’s Loadrunner.</td>
            </tr>
            <tr>
                <td>Quality Assurance</td>
                <td>Part of quality management focused on providing confidence that quality requirements will be fulfilled.</td>
            </tr>
            <tr>
                <td>Quality Management</td>
                <td>Coordinated activities to direct and control an organization with regard to quality. Direction and control with regard to quality generally includes the establishment of the quality policy and quality objectives, quality planning, quality control, quality assurance and quality improvement.</td>
            </tr>
            <tr>
                <td>Regression Testing</td>
                <td>Testing of a previously tested program following modification to ensure that defects have not been introduced or uncovered in unchanged areas of the software, as a result of the changes made. It is performed when the software or its environment is changed.</td>
            </tr>
            <tr>
                <td>Release Note</td>
                <td>A document identifying test items, their configuration, current status and other delivery information delivered by development to testing, and possibly other stakeholders, at the start of a test execution phase.</td>
            </tr>
            <tr>
                <td>Requirements-Based Testing </td>
                <td>An approach to testing in which test cases are designed based on test objectives and test conditions derived from requirements, e.g. tests that exercise specific functions or probe non-functional attributes such as reliability or usability.</td>
            </tr>
            <tr>
                <td>Review</td>
                <td>An evaluation of a product or project status to ascertain discrepancies from planned results and to recommend improvements. Examples include management review, informal review, technical review, inspection, and walkthrough</td>
            </tr>
            <tr>
                <td>Reviewer</td>
                <td>The person involved in the review that shall identify and describe anomalies in the product or project under review. Reviewers can be chosen to represent different viewpoints and roles in the review process.</td>
            </tr>
            <tr>
                <td>Risk</td>
                <td>A factor that could result in future negative consequences; usually expressed as impact and likelihood</td>
            </tr>
            <tr>
                <td>Risk-Based Testing</td>
                <td>Testing oriented towards exploring and providing information about product risks</td>
            </tr>
            <tr>
                <td>Risk Management</td>
                <td>Systematic application of procedures and practices to the tasks of identifying, analyzing, prioritizing, and controlling risk.</td>
            </tr>
            <tr>
                <td>Security</td>
                <td>Attributes of software products that bear on its ability to prevent unauthorized access, whether accidental or deliberate, to programs and data.</td>
            </tr>
            <tr>
                <td>Security Testing</td>
                <td>Testing to determine the security of the software product. This is performed at the infrastructure level at FCHP.</td>
            </tr>
            <tr>
                <td>Smoke Test</td>
                <td>A subset of all defined/planned test cases that cover the main functionality of a component or system, to ascertaining that the most crucial functions of a program work, but not bothering with finer details. A daily build and smoke test is among industry best practices.</td>
            </tr>
            <tr>
                <td>Specification</td>
                <td>The supplementary requirements are part of the software system specifications that cover the non-functional requirements that the software system needs to address.</td>
            </tr>
            <tr>
                <td>Supplementary Requirements</td>
                <td>A document that specifies, ideally in a complete, precise and verifiable manner, the requirements, design, behavior, or other characteristics of a component or system, and, often, the procedures for determining whether these provisions have been satisfied.<br />
                    Examples include:
                    <ul>
                        <li>Security features Scalability</li>
                        <li>Performance</li>
                        <li>Number of users supported, etc. </li>
                    </ul>
                    See also Non-Functional requirement
                </td>
            </tr>   
                <td>System Integration Testing</td>
                <td>Testing the integration of systems and packages; testing interfaces to external organizations (e.g. Electronic Data Interchange, Internet).</td>
            </tr>
            <tr>
                <td>Technical Review</td>
                <td>A peer group discussion activity that focuses on achieving consensus on the technical approach to be taken. A technical review is also known as a peer review.</td>
            </tr>
            <tr>
                <td>Test Approach</td>
                <td>The implementation of the test strategy for a specific project. It typically includes the decisions made that follow based on the (test) project’s goal and the risk assessment carried out, starting points regarding the test process, the test design techniques to be applied, exit criteria and test types to be performed.</td>
            </tr>
            <tr>
                <td>Test Automation</td>
                <td>The use of software to perform or support test activities, e.g. test management, test design, test execution and results checking.</td>
            </tr>
            <tr>
                <td>Test Case</td>
                <td>A set of input values, execution preconditions, expected results and execution post conditions, developed for a particular objective or test condition, such as to exercise a particular program path or to verify compliance with a specific requirement.</td>
            </tr>
            <tr>
                <td>Test Comparison</td>
                <td>The process of identifying differences between the actual results produced by the component or system under test and the expected results for a test.</td>
            </tr>
            <tr>
                <td>Test Condition</td>
                <td>An item or event of a component or system that could be verified by one or more test cases, e.g. a function, transaction, quality attribute, or structural element.</td>
            </tr>
            <tr>
                <td>Test Data</td>
                <td>Data that exists (for example, in a database) before a test is executed, and that affects or is affected by the component or system under test</td>
            </tr>
            <tr>
                <td>Test Design Specification</td>
                <td>A document specifying the test conditions (coverage items) for a test item, the detailed test approach and identifying the associated high level test cases.</td>
            </tr>
            <tr>
                <td>Test Environment</td>
                <td>An environment containing hardware, instrumentation, simulators, software tools, and other support elements needed to conduct a test.</td>
            </tr>
            <tr>
                <td>Test Execution</td>
                <td>The process of running a test by the component or system under test, producing actual result(s).</td>
            </tr>
            <tr>
                <td>Test Execution Automation</td>
                <td>The use of software, e.g. capture/playback tools, to control the execution of tests, the comparison of actual results to expected results, the setting up of test preconditions, and other test control and reporting functions</td>
            </tr>
            <tr>
                <td>Test Execution Phase</td>
                <td>The period of time in a software development life cycle during which the components of a software product are executed, and the software product is evaluated to determine whether or not requirements have been satisfied.</td>
            </tr>
            <tr>
                <td>Test Execution Schedule</td>
                <td>A scheme for the execution of test procedures. The test procedures are included in the test execution schedule in their context and in the order in which they are to be executed</td>
            </tr>
            <tr>
                <td>Test Execution Tool</td>
                <td>A type of test tool that is able to execute other software using an automated test script, e.g. capture/playback.</td>
            </tr>
            <tr>
                <td>Test Infrastructure</td>
                <td>The organizational artifacts needed to perform testing, consisting of test environments, test tools, office environment and procedures.</td>
            </tr>
            <tr>
                <td>Test Level</td>
                <td>A group of test activities that is organized and managed together. A test level is linked to the responsibilities in a project. Examples of test levels are component test, integration test, system test and acceptance test</td>
            </tr>
            <tr>
                <td>Test Logs</td>
                <td>A chronological record of relevant details about the execution of test</td>
            </tr>
            <tr>
                <td>Test Manager</td>
                <td>The person responsible for testing and evaluating a test object. The individual, who directs, controls, administers plans and regulates the evaluation of a test object.</td>
            </tr>
            <tr>
                <td>Test Management</td>
                <td>The planning, estimating, monitoring and control of test activities, typically carried out by a test manager.</td>
            </tr>
            <tr>
                <td>Test Process Improvement</td>
                <td>(TPI): A continuous framework for test process improvement that describes the key elements of an effective test process, especially targeted at system testing and acceptance testing.</td>
            </tr>
            <tr>
                <td>Test Objective</td>
                <td>A reason or purpose for designing and executing a test.</td>
            </tr>
            <tr>
                <td>Test Phase</td>
                <td>A distinct set of test activities collected into a manageable phase of a project, e.g. the execution activities of a test level.</td>
            </tr>
            <tr>
                <td>Test Plan</td>
                <td>A document describing the scope, approach, resources and schedule of intended test activities. It identifies amongst others test items, the features to be tested, the testing tasks, who will do each task, degree of tester independence, the test environment, the test design techniques and test measurement techniques to be used, and the rationale for their choice, and any risks requiring contingency planning. It is a record of the test planning process.</td>
            </tr>
            <tr>
                <td>Test Planning</td>
                <td>The activity of establishing or updating a test plan.</td>
            </tr>
            <tr>
                <td>Test Process</td>
                <td>The fundamental test process comprises planning, specification, execution, recording and checking for completion.</td>
            </tr>
            <tr>
                <td>Test Repeatability</td>
                <td>An attribute of a test indicating whether the same results are produced each time the test is executed.</td>
            </tr>
            <tr>
                <td>Test Run</td>
                <td>Execution of a test on a specific version of the test object.</td>
            </tr>
            <tr>
                <td>Test Script</td>
                <td>Commonly used to refer to a test procedure specification, especially an automated one.</td>
            </tr>
            <tr>
                <td>Test Specification</td>
                <td>A document that consists of a test design specification, test case specification and/or test procedure specification.</td>
            </tr>
            <tr>
                <td>Test Strategy</td>
                <td>A high-level document defining the test levels to be performed and the testing within those levels for a programmed (one or more projects).</td>
            </tr>
            <tr>
                <td>Test Summary Report</td>
                <td> </td>
            </tr>
            <tr>
                <td>Test Tool</td>
                <td>A software product that supports one or more test activities, such as planning and control, specification, building initial files and data, test execution and test analysis such as HPQC.</td>
            </tr>
            <tr>
                <td>Test Type</td>
                <td>A group of test activities aimed at testing a component or system regarding one or more interrelated quality attributes. A test type is focused on a specific test objective, i.e. reliability test, usability test, regression test etc., and may take place on one or more test levels or test phases.</td>
            </tr>
            <tr>
                <td>Tester</td>
                <td>A technically skilled professional who is involved in the testing of a component or system.</td>
            </tr>
            <tr>
                <td>Testing</td>
                <td>The process consisting of all life cycle activities, both static and dynamic, concerned with planning, preparation and evaluation of software products and related work products to determine that they satisfy specified requirements, to demonstrate that they are fit for purpose and to detect defects.</td>
            </tr>
            <tr>
                <td>Test Ware</td>
                <td>Artifacts produced during the test process required to plan, design, and execute tests, such as documentation, scripts, inputs, expected results, set-up and clear-up procedures, files, databases, environment, and any additional software or utilities used in testing.</td>
            </tr>
            <tr>
                <td>Traceability</td>
                <td>The ability to identify related items in documentation and software, such as requirements with associated tests. See also horizontal traceability, vertical traceability.</td>
            </tr>
            <tr>
                <td>User Acceptance Testing</td>
                <td>A group representing a cross section of end users tests the application. The user acceptance testing is done using real world scenarios and perceptions relevant to the end users.</td>
            </tr>
            <tr>
                <td>Validation</td>
                <td>Confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled.</td>
            </tr>
            <tr>
                <td>Verification</td>
                <td>Confirmation by examination and through the provision of objective evidence that specified requirements have been fulfilled.</td>
            </tr>
            <tr>
                <td>Walkthrough</td>
                <td>A step-by-step presentation by the author of a document in order to gather information and to establish a common understanding of its content.</td>
            </tr>
            <tr>
                <td>Unit Testing</td>
                <td>A level of the software testing process where individual units/components of a software/system are tested. The purpose is to validate that each unit of the software performs as designed. Unit Testing is normally performed by software developers themselves or their peers.</td>
            </tr>
        </table>
        <small><a href="#top">contents</a></small>
 
</div>