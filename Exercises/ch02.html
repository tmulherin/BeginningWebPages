<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="UTF-8"/>
        <title>User Acceptance Testing for Beginners</title>
    </head>
    <body>
        <a href="../first_page.html">Back</a>
    <h3 align="center">User Acceptance Testing for Beginners</h3> 
                <h4 align="center">– A handbook</h4>

            
    


















Table of Contents
User Acceptance Testing (UAT) Overview  3
Test Artifacts  3
Glossary of terms   3
Test Phases 9
Test Artifacts and Owners   9
UAT Test Resources  10
Tasks Preceding UAT 11
UAT Entrance Criteria   11
Test Preparation / Test Documentation   12
UAT Testing 12
Defect Management   13
Defect Management, cont.    14
UAT Testing Evidence    14
Go/No Go    14
UAT Communication Plan  15
Business Checkout - Test Preparation / Test Documentation   15
Release Housekeeping    15
Where should I be testing?  15
















User Acceptance Testing (UAT) Overview
User Acceptance Testing is very important to the Business.  It helps ensure successful delivery of new applications and/or functionality by testing the applications in the context of the business environment in which it will operate.  The expectation is that if software works as intended during simulation of normal use, it will work the same in production.  
Application end-users perform UAT.  They understand the business and can effectively evaluate if application changes function as expected during normal business usage.   
UAT is designed to verify performance of critical business functions and integrity of converted and additional data prior to checkout weekend when system changes are moved to production.  UAT is performed after QA has completed tests designed to flush out show stoppers such as system crashes.  The business end-user sign offs that the system is ready to “Go Live”.
Test Artifacts
QA develops the following test artifacts 2-3 months in advance of scheduled releases based on size and complexity of release.  There are three categories of artifacts.

a)  The test strategy outlines how testing for the release will be accomplished.  This includes:  technical tools/environments; necessary test data and its creation; business scenarios (provided by the project Business SMEs); and identification of necessary test cases.  If help is required to get the UAT Testers up to speed on changes based on the number and/or complexity of changes, this will be specifically outlined in the strategy. It is generally a high level document that identifies “what” you are going to test.

b)  The test plan outlines the test phases, associated tasks, and assigned resources. It is more detailed and generally defines “how” you are going to test.

c)  The test cases outline each step necessary to execute the testing and specify actions and expected results.  QA generally performs walkthroughs with the business SMEs to ensure all major functions are covered.

The inputs for the creation of these documents would be based on use cases, business requirements, technical specifications and any review comments from the business team/SME.
QA reviews the test plan with the core project team, makes necessary updates, and receives sign off from the core team.  The application Project Manager then communicates the plan to all affected parties.
Glossary of terms
Acceptance Criteria - The exit criteria that a component or system must satisfy in order to be accepted by a user, customer, or other authorized entity. 
Acceptance Testing - Formal testing with respect to user needs, requirements, and business processes conducted to determine whether or not a system satisfies the acceptance criteria and to enable the user, customers or other authorized entity to determine whether or not to accept the system. 
Actual Result - The behavior produced/observed when a component or system is tested.
Anomaly - Any condition that deviates from expectation based on requirements specifications, design documents, user documents, standards, etc. or from someone’s perception or experience. Anomalies may be found during, but not limited to, reviewing, testing, analysis, compilation, or use of software products or applicable documentation.


Audit Trail - A path by which the original input to a process (e.g. data) can be traced back through the process, taking the process output as a starting point. This facilitates defect analysis and allows a process audit to be carried out
Behavior - The response of a component or system to a set of input values and preconditions
Best Practice - A superior method or innovative practice that contributes to the improved performance of an organization under given context, usually recognized as ‘best’ by other peer organizations.
Beta Testing - Operational testing by potential and/or existing users/customers at an external site not otherwise involved with the developers, to determine whether or not a component or system satisfies the user/customer needs and fits within the business processes. Beta testing is often employed as a form of external acceptance testing in order to acquire feedback from the market.
Black Box Testing - Testing, either functional or non-functional, without reference to the internal structure of the component or system.
Business Process-Based Testing - An approach to testing in which test cases are designed based on descriptions and/or knowledge of business processes.
Capability Maturity Model (CMM) - A five level staged framework that describes the key elements of an effective software process. The Capability Maturity Model covers practices for planning, engineering and managing software development and maintenance. 
Certification - The process of confirming that a component, system or person complies with its specified requirements, e.g. by passing an exam.
CCB - Change Control Board – controlling body for changes in scope
Compliance testing - The process of testing to determine the compliance of a component or system.
Configuration - The composition of a component or system as defined by the number, nature, and interconnections of its constituent parts.
Configuration Management - A discipline applying technical and administrative direction and surveillance to: identify and document the functional and physical characteristics of a configuration item, control changes to those characteristics, record and report change processing and implementation status, and verify compliance with specified requirements.
Coverage - The degree, expressed as a percentage, to which a specified coverage item has been exercised by a test suite.
Data Driven Testing - A scripting technique that stores test input and expected results in a table or spreadsheet, so that a single control script can execute all of the tests in the table. Data driven testing is often used to support the application of test execution tools such as capture/playback tools(automation). 
Data Validation Testing - The checking of data for correctness, or the determination of compliance with applicable standards, rules, and conventions. 
Defect - A flaw in a component or system that can cause the component or system to fail to perform its required function, e.g. an incorrect statement or data definition. A defect, if encountered during execution, may cause a failure of the component or system. 
Defect Management - The process of recognizing, investigating, taking action and disposing of defects. It involves recording defects, classifying them and identifying the impact. 

Entry Criteria - The set of generic and specific conditions for permitting a process to go forward with a defined task, e.g. test phase. The purpose of entry criteria is to prevent a task from starting which would entail more (wasted) effort compared to the effort needed to remove the failed entry criteria. 
Exit Criteria - The set of generic and specific conditions, agreed upon with the stakeholders, for permitting a process to be officially completed. The purpose of exit criteria is to prevent a task from being considered completed when there are still outstanding parts of the task which have not been finished. Exit criteria are used by testing to report against and to plan when to stop testing. 
Expected Result - The behavior predicted by the specification, or another source, of the component or system under specified conditions.
Fail - A test is deemed to fail if its actual result does not match its expected result
Formal Review - A review characterized by documented procedures and requirements, e.g. inspection.
Functional Requirement - A requirement that specifies a function that a component or system must perform.
Functional Testing - Testing based on an analysis of the specification of the functionality of a component or system.
Integration Testing - Testing performed to expose defects in the interfaces and in the interactions between integrated components or systems. See also component integration testing, system integration testing.
Metric - A measurement scale and the method used for measurement
Milestone - A point in time in a project at which defined (intermediate) deliverables and results should be ready
Non-Functional Requirement - A requirement that does not relate to functionality, but to attributes of such as reliability, efficiency, usability, maintainability and portability.
Pass - A test is deemed to pass if its actual result matches its expected result.
Priority - The level of (business) importance assigned to an item, e.g. defect
Performance testing - The process of testing to determine the performance of a software product. Testing is conducted using some type of performance tool such as Rational Performance Tester or HP’s Loadrunner.
Quality Assurance - Part of quality management focused on providing confidence that quality requirements will be fulfilled
Quality Management - Coordinated activities to direct and control an organization with regard to quality. Direction and control with regard to quality generally includes the establishment of the quality policy and quality objectives, quality planning, quality control, quality assurance and quality improvement. 
Regression Testing - Testing of a previously tested program following modification to ensure that defects have not been introduced or uncovered in unchanged areas of the software, as a result of the changes made. It is performed when the software or its environment is changed.
Release Note - A document identifying test items, their configuration, current status and other delivery information delivered by development to testing, and possibly other stakeholders, at the start of a test execution phase. 


Requirements-Based Testing - An approach to testing in which test cases are designed based on test objectives and test conditions derived from requirements, e.g. tests that exercise specific functions or probe non-functional attributes such as reliability or usability.
Review - An evaluation of a product or project status to ascertain discrepancies from planned results and to recommend improvements. Examples include management review, informal review, technical review, inspection, and walkthrough
Reviewer - The person involved in the review that shall identify and describe anomalies in the product or project under review. Reviewers can be chosen to represent different viewpoints and roles in the review process.
Risk - A factor that could result in future negative consequences; usually expressed as impact and likelihood
Risk-Based Testing - Testing oriented towards exploring and providing information about product risks
Risk Management - Systematic application of procedures and practices to the tasks of identifying, analyzing, prioritizing, and controlling risk.
Security - Attributes of software products that bear on its ability to prevent unauthorized access, whether accidental or deliberate, to programs and data. 
Security Testing - Testing to determine the security of the software product. This is performed at the infrastructure level at FCHP.
Smoke Test - A subset of all defined/planned test cases that cover the main functionality of a component or system, to ascertaining that the most crucial functions of a program work, but not bothering with finer details. A daily build and smoke test is among industry best practices. 
Specification - A document that specifies, ideally in a complete, precise and verifiable manner, the requirements, design, behavior, or other characteristics of a component or system, and, often, the procedures for determining whether these provisions have been satisfied. 
Supplementary Requirements - The supplementary requirements are part of the software system specifications that cover the non-functional requirements that the software system needs to address.
Examples include:
•   Security features Scalability
•   Performance
•   Number of users supported, etc. 
See also Non-Functional requirement
System Integration Testing - Testing the integration of systems and packages; testing interfaces to external organizations (e.g. Electronic Data Interchange, Internet).
Technical Review - A peer group discussion activity that focuses on achieving consensus on the technical approach to be taken. A technical review is also known as a peer review. 


Test Approach - The implementation of the test strategy for a specific project. It typically includes the decisions made that follow based on the (test) project’s goal and the risk assessment carried out, starting points regarding the test process, the test design techniques to be applied, exit criteria and test types to be performed. 
Test Automation - The use of software to perform or support test activities, e.g. test management, test design, test execution and results checking.
Test Case - A set of input values, execution preconditions, expected results and execution post conditions, developed for a particular objective or test condition, such as to exercise a particular program path or to verify compliance with a specific requirement.
Test Comparison - The process of identifying differences between the actual results produced by the component or system under test and the expected results for a test.
Test Condition - An item or event of a component or system that could be verified by one or more test cases, e.g. a function, transaction, quality attribute, or structural element.
Test Data - Data that exists (for example, in a database) before a test is executed, and that affects or is affected by the component or system under test
Test Design Specification - A document specifying the test conditions (coverage items) for a test item, the detailed test approach and identifying the associated high level test cases.
Test Environment - An environment containing hardware, instrumentation, simulators, software tools, and other support elements needed to conduct a test.
Test Execution - The process of running a test by the component or system under test, producing actual result(s).
Test Execution Automation - The use of software, e.g. capture/playback tools, to control the execution of tests, the comparison of actual results to expected results, the setting up of test preconditions, and other test control and reporting functions
Test Execution Phase -  The period of time in a software development life cycle during which the components of a software product are executed, and the software product is evaluated to determine whether or not requirements have been satisfied.
Test Execution Schedule - A scheme for the execution of test procedures. The test procedures are included in the test execution schedule in their context and in the order in which they are to be executed
Test Execution Tool - A type of test tool that is able to execute other software using an automated test script, e.g. capture/playback. 
Test Infrastructure - The organizational artifacts needed to perform testing, consisting of test environments, test tools, office environment and procedures.
Test Level - A group of test activities that is organized and managed together. A test level is linked to the responsibilities in a project. Examples of test levels are component test, integration test, system test and acceptance test
Test Logs - A chronological record of relevant details about the execution of test

Test Manager -  The person responsible for testing and evaluating a test object. The individual, who directs, controls, administers plans and regulates the evaluation of a test object.
Test Management - The planning, estimating, monitoring and control of test activities, typically carried out by a test manager.
Test Process Improvement - (TPI): A continuous framework for test process improvement that describes the key elements of an effective test process, especially targeted at system testing and acceptance testing.
Test Objective - A reason or purpose for designing and executing a test.
Test Phase - A distinct set of test activities collected into a manageable phase of a project, e.g. the execution activities of a test level. 
Test Plan - A document describing the scope, approach, resources and schedule of intended test activities. It identifies amongst others test items, the features to be tested, the testing tasks, who will do each task, degree of tester independence, the test environment, the test design techniques and test measurement techniques to be used, and the rationale for their choice, and any risks requiring contingency planning. It is a record of the test planning process.
Test Planning - The activity of establishing or updating a test plan.
Test Process - The fundamental test process comprises planning, specification, execution, recording and checking for completion.
Test Repeatability - An attribute of a test indicating whether the same results are produced each time the test is executed.
Test Run - Execution of a test on a specific version of the test object.
Test Script - Commonly used to refer to a test procedure specification, especially an automated one.
Test Specification - A document that consists of a test design specification, test case specification and/or test procedure specification.
Test Strategy - A high-level document defining the test levels to be performed and the testing within those levels for a programmed (one or more projects)
Test Summary Report -   A document summarizing testing activities and results. It also contains an evaluation of the corresponding test items against exit criteria.  
Test Tool - A software product that supports one or more test activities, such as planning and control, specification, building initial files and data, test execution and test analysis such as HPQC.
Test Type - A group of test activities aimed at testing a component or system regarding one or more interrelated quality attributes. A test type is focused on a specific test objective, i.e. reliability test, usability test, regression test etc., and may take place on one or more test levels or test phases. 
Tester - A technically skilled professional who is involved in the testing of a component or system.
Testing - The process consisting of all life cycle activities, both static and dynamic, concerned with planning, preparation and evaluation of software products and related work products to determine that they satisfy specified requirements, to demonstrate that they are fit for purpose and to detect defects.

Test Ware - Artifacts produced during the test process required to plan, design, and execute tests, such as documentation, scripts, inputs, expected results, set-up and clear-up procedures, files, databases, environment, and any additional software or utilities used in testing. 
Traceability - The ability to identify related items in documentation and software, such as requirements with associated tests. See also horizontal traceability, vertical traceability.
User Acceptance Testing - A group representing a cross section of end users tests the application. The user acceptance testing is done using real world scenarios and perceptions relevant to the end users.
Validation - Confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled. 
Verification - Confirmation by examination and through the provision of objective evidence that specified requirements have been fulfilled.
Walkthrough - A step-by-step presentation by the author of a document in order to gather information and to establish a common understanding of its content.
Unit Testing - Unit Testing is a level of the software testing process where individual units/components of a software/system are tested. The purpose is to validate that each unit of the software performs as designed. Unit Testing is normally performed by software developers themselves or their peers.
Test Phases
There are 4 phases of testing:  Unit; Systems; Acceptance; and Release.
Phase   Tests   Owner/Executor  Purpose
Unit Test   Usability   Developers  Validate design meets user goals and ease of use
    Unit    Developers  Test individual modules & programs
Systems Test    Function    QA  Use non-integrated data to validate changes made to an application are functioning correctly.
    Integration QA  Use integrated data to validate changes work with impacted applications & ensure applications interfacing correctly.
    Regression  QA  Evaluate interactions between components to determine impact of changes on target environments.
Acceptance Test Acceptance  Business user acceptance testers    Validate that new business functionality & applications work correctly under normal business workflows.
Release Test    Release QA  Use frozen code to validate that new and existing functionality work correctly.
    Business Checkout   UW - QA w/ business backup
Sales – Sales Effectiveness Validate that new business functionality & applications have been successfully moved to production.
Test Artifacts and Owners



In order to perform the required testing, artifacts are required.  The below table outlines this information by test phase
Phase   Tests   Inputs  Owner / Creator Outputs Owner   Sign-off
Unit Test   Usability   Test Cases  Developers      QA  QA
    Unit    Test Cases  Developers      QA  QA
Systems Test    Function    Test Cases  QA  Test results    QA  QA
    Integration Test Cases  QA  Test results    QA  QA
    Regression  Test Cases  QA  Test results    QA  Business SME
Acceptance Test User Acceptance Testing Test Scenarios and Cases    Business SME    Test results    Business/UAT Testers    Business SME
Release Test    Release Test Cases  QA      QA  
    Business Checkout   Test Scenarios and Cases    Business SME    Test results    QA  Business SME
UAT Test Resources
The Business resources performing UAT will vary depending on the application and functionality:

Note: We are in the process of identifying/validating the business resources for each application.  
Tasks Preceding UAT 
Prior to executing User Acceptance Testing, the following needs to be completed:
i.  Data Setup
Task:  Populate the test environment with test data based on business scenarios provided by the project Business Application SMEs and outlined in the QA test plan.  Examples include:  accounts, opportunities, and quotes. 
Lead Time:  TBD
Owner:  QA
ii. Access to Applications

Task:  Ensure UAT test ids are set up with the appropriate rights.  
Lead Time:  TBD
Owner:  QA and Tech Services Team
Pre-requisite: SME to provide the list of UAT testers to QA.

Task:  Ensure testers have access to the applications being tested
Lead Time:  TBD
Owner:  Business Application SME

Task:  Ensure UAT Test Coordinator has access to the application defect tracker
Lead Time:  TBD

Owner:  Business Application SME
Note: Each of the above activities would be listed as activities within the QA project plan
UAT Entrance Criteria
User Acceptance testing is typically performed near the end of the test phase. It is done once the QA testers have completed their functional, integration, and the majority of the regression testing and presented test evidence to the Business application subject matter experts (SMEs) for sign-off.  QA will send an email notification to User Acceptance testers to start testing.
   It is best to not have any open critical or major severity defects.  Assessment of open defects must be made and documented to determine UAT GO/NO GO decision.
   A defect log has been created.  Next steps on all defects have been agreed upon by the business and IT.  
   A Defect Risk Assessment of defects has been performed and documented.
   Outstanding issues pertaining to the Systems Test phases and an Issues Risk Assessment has been performed and documented.
   Key Systems Development Life Cycle (SDLC) deliverables impacted by the System Test phase have been updated:
-   Business Requirements
-   Technical Design
-   Test Strategy
-   Test Plan 
-   Issues Log
-   Risk Assessment
-   Integrated Test Cases
-   System Test Cases
   Acceptance Environment is stable.
   All required code for the release has been deployed to the Acceptance Test environment.
   Access to the UAT applications for the UAT testers has been established.
Test Preparation / Test Documentation 
Task:  Plan a UAT Training session
Lead Time:  TBD
Owner:  Business UAT Lead

Task:  Write high level business summary (ex. Release notes) of the application changes being implemented in the release. 
Lead Time:  TBD
Owner:  Business Application SME and SA 

Task:  Prepare for design review with UAT testers (ex. Demo of application changes)
Lead Time:  TBD
Owner:  SA

Task:  User Acceptance Testers participate in a release training session.  Training session will include: 
   Review of UAT expectations 
   Review of UAT Handbook 
   SA presentation of the changes to the application(s) allowing User Acceptance Testers to ask questions and view the application in the test environment.  
   Break out sessions for identifying business scenarios to test the range of old and new functionality.  Supported by SA, QA, and Business Application SMEs. Note: The business scenarios will be used to create the test data in the test environments.  Refer to Data Setup in the previous section.


   Prioritization of business scenarios identifying which scenarios are critical to the success of the release.  
   Development of test cases that identify the functionality that will be tested, including the new functionality.  As appropriate, include integrated end to end test cases across roles and applications.
Sample:  
 
Lead Time:  TBD
Owner:  Business UAT Lead, Business Application SME’s, SA’s, QA, and UAT Testers
UAT Testing
The extent of UAT depends on the type of release and will be performed by the user acceptance testers:
1.  Scheduled Release – Any new functionality or pre-defined set of enhancements /defect fixes.
For scheduled releases testing is detailed and typically entails:
•   Thorough testing of the new business functionality.
•   Validating that enhancements haven’t broken existing critical business flows that Sales/Underwriting uses to fulfill the business commitments
2.  Hot Fix Release – Mitigate or fix critical issues. 
Testing for hot fixes are typically less comprehensive and entails: 
•   Thorough testing of the actual fixes going in 
•   A quick check of existing critical business flows that Sales/Underwriting uses to fulfill the business commitments
Defect Management
Defect management is a critical part of UAT.  Defects are defined as any function/feature of the application that does not meet the defined specifications.  Enhancements are requests for functionality in line with current specifications but improve business flow or performance.  
Defects will be triaged in a daily application defect meeting and should be tracked to closure.  They will either get fixed or be deferred to a future release.  More complex releases will require an overall release triage meeting that will meet as needed.  This meeting is designed to identify critical dependencies across applications in order to appropriately prioritize defect resolution.
Defect Tracking
Task:  Report defects to the Business Application SME.  Assign Severity and Priority to each defect.
Sample:  
 
Definitions:  Severity:
o   1 – Critical: the defect has caused all testing of that particular test script to cease; the defect must be fixed ASAP.
o   2 – Major: the defect has a major impact on the system; however, there is a temporary work around such that testing does not have to cease; the defect should be fixed ASAP.
o   3 – Minor: the defect has a minor impact on the system; however, there is a work around and testing does not have to cease.  

o   4 – Cosmetic: the defect requires a “cosmetic” or “nice to have” fix.
Owner:  User Acceptance Tester
Task:  Validate and consolidate defects, adjust severity and priority as appropriate at the application level, log in defect tracking tool, and report to QA Lead
Owner:  Business Application SME
Task:  Track application defects in TBD
Owner:  QA Lead
Triage Meetings
Task:  Schedule defect triage meetings with application PM, Lead Developer, SA Lead, and Business Application SME
Owner:  QA Lead
Task:  Facilitate defect triage meetings to validate defects, assess development effort, prioritize defects, and track decisions in TBD.  Note: Any e-mail threads that occur outside of the triage meeting regarding defects should be kept to a minimum.  When required to facilitate resolution, the QA lead should be copied in on the messages so that the decision thread can be captured in Mercury Quality Center.  It is best practice to call a meeting when the communication requires more than three participants and more than three messages.
Owner:  QA Lead
Defect Management, cont.
Triage Meetings, cont.
Task:  Attend QA defect triage meetings, frequency of which depends on number of defects being found, and report out to User Acceptance Testers.
Owner:  Application PM, Lead Developer, and Business Application SME
Task:  Schedule overall portfolio release defect triage meetings with application PM’s, Lead Developers, QA Leads, SA Lead, and Business Application SME’s
Owner:  Coordinated Release QA Lead
Task:  Facilitate overall portfolio release defect triage meetings to assess critical and major severity defects with application interdependencies, assess development effort, prioritize defects, and track decisions in Mercury Quality Center.  
Owner:  Coordinated Release QA Lead
Task:  Attend QA overall release defect triage meetings, frequency of which depends on number of defects being found, and report out to User Acceptance Testers.
Owner:  Application PM’s, Lead Developers, QA Leads, and Business Application SME’s
Note:  If the application release is part of a larger Integrated Release, the Business Application SME will partner closely with Business Integrated Release SME on defect management.
UAT Testing Evidence
It is important for User Acceptance Testers to provide QA and the Business Application SME’s with evidence that they performed the required testing.  This is will enable Business Application SME’s to provide sign-off on testing and provide QA with backup data to recreate test scenarios when needed.
Task:  Provide documentation of test cases run and test data used
Owner:  User Acceptance Testers

Task:  Review documentation of test cases run and test data used provided by each User Acceptance Tester and validate that testing is complete
Owner:  Business Application SME
Go/No Go
Task: User Acceptance Sign-off
Once user acceptance testing is complete and validated, the Business Application SME will send an email sign-off to the application PM and QA Lead
Owner: Business Application SME
Task: Go/No-Go
The successful exit criteria for UAT would be that there are no Critical or Major defects left unresolved for the release. The Business owners would review the open defects list for the release and then determine the GO/NO-GO decision.  A GO decision indicates that the application/module is deemed ready to be moved to production.  A NO-GO decision means that the application/module can not be moved to production with the Critical or Major defects left unresolved.
Owner: Business Application SME’s + Application PM’s
UAT Communication Plan
Communication amongst stakeholders is critical to the success of any release.  Attached is a communication plan that provides guidance on communicating during the UAT test phase.
 
Business Checkout - Test Preparation / Test Documentation 
Small Change / Small Complexity:

Task:  Create business scenarios and test cases for checkout that identify the functionality that will be tested, including the new functionality.  QA will execute the checkout test cases and UW will provide Sign-off after reviewing the QA results.
Owner:  QA / Business Application SME

Medium or Large Change / Medium or Large Complexity:

Task:  Create business scenarios and test cases for checkout that identify the functionality that will be tested, including the new functionality.  These test cases will be executed by QA. The QA team will check for any show stoppers and then give the Business SME’s the go-ahead for business checkout of particular project changes.
Owner:  QA / Business Application SME
Release Housekeeping
Once UAT is complete, QA will document any results for future reference and will send updates of the business scenarios and scripts to the Business Application SME.
Where should I be testing?
        <a href="../first_page.html">Back</a>
    </body>
</html>